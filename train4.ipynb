{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ridaIndah/Proyek-Mandiri/blob/main/train4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jr35WLJRuPlO"
      },
      "source": [
        "# **cek Cuda (True or False)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awQ9G_OLtZuh",
        "outputId": "6870f531-9d26-4302-ad2e-f454ef18248f"
      },
      "source": [
        "import torch\n",
        "torch.cuda.is_available() #should be true"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4xID-EWrdE_",
        "outputId": "561338aa-3fdc-42c6-e57e-0e514578d0b5"
      },
      "source": [
        "!nvcc --version"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2020 NVIDIA Corporation\n",
            "Built on Wed_Jul_22_19:09:09_PDT_2020\n",
            "Cuda compilation tools, release 11.0, V11.0.221\n",
            "Build cuda_11.0_bu.TC445_37.28845127_0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EAxjod9R0ID"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHuG2jqitZEY",
        "outputId": "bb6ee983-2b90-454f-d5d6-3642837c9452"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Jun  5 10:19:17 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.27       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKFYMgbB3W8g",
        "outputId": "2d0671cf-7716-4204-a336-6ca2c02f3b1c"
      },
      "source": [
        "torch.backends.cudnn.is_available()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0kcSl6Ll7cf",
        "outputId": "d5dc6f3c-1557-4e86-8b9d-5447794767a2"
      },
      "source": [
        "#install pymagnitude for word embedding\n",
        "pip install pymagnitude"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pymagnitude\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/40/ec05f0630f6dd56eb323dee5bc70a52e4f21247185d8662d3d08267cc6c7/pymagnitude-0.1.143.tar.gz (5.4MB)\n",
            "\u001b[K     |████████████████████████████████| 5.4MB 6.5MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pymagnitude\n",
            "  Building wheel for pymagnitude (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pymagnitude: filename=pymagnitude-0.1.143-cp37-cp37m-linux_x86_64.whl size=360640573 sha256=5f2993f2118f72e468b2691aa6e937857be70e34619eec8a159adb4233838b6c\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/b2/16/f88eb50800667ffe23d509c7e2157923a08d10ed6d6410124f\n",
            "Successfully built pymagnitude\n",
            "Installing collected packages: pymagnitude\n",
            "Successfully installed pymagnitude-0.1.143\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHtqKjL6lXFP"
      },
      "source": [
        "import os\n",
        "import random\n",
        "import logging\n",
        "import argparse\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.nn.init import xavier_uniform_\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yd4WUUxH8VQl"
      },
      "source": [
        "from pymagnitude import Magnitude"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cktmOenwtXxn"
      },
      "source": [
        "# Prepare data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-Mgkzu9oR_E"
      },
      "source": [
        "\n",
        "def load_pickle(fname):\n",
        "    with open(fname, \"rb\") as f:\n",
        "        return pickle.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bssDhxelxcg",
        "outputId": "2ce6a138-5716-41a8-e117-7332818e451f"
      },
      "source": [
        "#for preproses data DD\n",
        "import re\n",
        "import csv\n",
        "import random\n",
        "from collections import Counter, defaultdict\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS as spacy_stopwords\n",
        "\n",
        "\n",
        "DD_label_to_emotions = {0: \"neutral\", 1: \"anger\", 2: \"disgust\", 3: \"fear\", 4: \"happiness\", 5: \"sadness\", 6: \"surprise\"}\n",
        "nltk_stopwords = stopwords.words('english')\n",
        "# spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS # older version of spacy\n",
        "stopwords = set(nltk_stopwords).union(spacy_stopwords)\n",
        "porter = PorterStemmer()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulqpcvzbqXOl"
      },
      "source": [
        "#Define class Vocab for building vocabulary\n",
        "class Vocab(object):\n",
        "    def __init__(self, examples, min_freq, max_vocab_size):\n",
        "        \"\"\"\n",
        "            examples: a list of examples, each example is a list of (utter, speaker, emotion, mask), followed by an emotion label\n",
        "            min_freq: the min frequency of word in the vocabulary\n",
        "            max_vocab_size: max vocabulary size\n",
        "            return: a Vocab object\n",
        "        \"\"\"\n",
        "        self.min_freq = min_freq\n",
        "        self.max_vocab_size = max_vocab_size\n",
        "        \n",
        "        self.word2id = {\"<pad>\": 0, \"<unk>\": 1}\n",
        "        self.id2word = {0: \"<pad>\", 1: \"<unk>\"}\n",
        "        self.speaker2id = {}\n",
        "        self.emotion2id = {}\n",
        "\n",
        "        self.word_freq_dist = Counter()\n",
        "        self.speaker_freq_dist = Counter()\n",
        "        self.emotion_freq_dist = Counter()\n",
        "        \n",
        "        utterance_lengths = []\n",
        "        conversation_lengths = []\n",
        "\n",
        "        for ex in examples:\n",
        "            conv_length = sum([utter[-1] for utter in ex])\n",
        "            conversation_lengths.append(conv_length)\n",
        "            for utter, speaker, emotion, mask in ex:\n",
        "                if mask == 1:\n",
        "                    self.word_freq_dist.update(utter)\n",
        "                    self.speaker_freq_dist.update([speaker])\n",
        "                    self.emotion_freq_dist.update([emotion])\n",
        "                    utterance_lengths.append(len(utter))\n",
        "\n",
        "        # filter by min_freq\n",
        "        words = [(w,cnt) for w, cnt in self.word_freq_dist.items() if cnt >= min_freq]\n",
        "        words = sorted(words, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # cap vocab size\n",
        "        words = words[:max_vocab_size]\n",
        "\n",
        "        # build word2id and id2word\n",
        "        for idx, (w, cnt) in enumerate(words):\n",
        "            self.word2id[w] = idx+2\n",
        "            self.id2word[idx+2] = w\n",
        "\n",
        "        # build speaker to id\n",
        "        speakers = sorted(self.speaker_freq_dist.items(), key=lambda x: x[1], reverse=True)\n",
        "        for idx, (speaker, cnt) in enumerate(speakers):\n",
        "            self.speaker2id[speaker] = idx\n",
        "\n",
        "        # build emotion to id\n",
        "        emotions = sorted(self.emotion_freq_dist.items(), key=lambda x: x[1], reverse=True)\n",
        "        for idx, (emotion, cnt) in enumerate(emotions):\n",
        "            self.emotion2id[emotion] = idx\n",
        "        \n",
        "        self.max_conversation_length = max(conversation_lengths)\n",
        "        self.max_sequence_length = max(utterance_lengths)\n",
        "        self.num_utterances = len(utterance_lengths)\n",
        "        \n",
        "\n",
        "def convert_examples_to_ids(examples, vocab):\n",
        "    \"\"\"\n",
        "        examples: a list of examples, each example is a list of (utter, speaker, emotion, mask), followed by an emotion label\n",
        "        vocab: a Vocab object\n",
        "        max_sequence_length: max sequence length\n",
        "        \n",
        "        return: examples containing word ids\n",
        "    \"\"\"\n",
        "    examples_ids = []\n",
        "    for ex in examples:\n",
        "        ex_ids = []\n",
        "        for utter, speaker, emotion, mask in ex:\n",
        "            utter_ids = [vocab.word2id[w] if w in vocab.word2id else vocab.word2id[\"<unk>\"] for w in utter]\n",
        "            utter_ids = utter_ids + (vocab.max_sequence_length - len(utter_ids)) * [vocab.word2id[\"<pad>\"]]\n",
        "            if speaker in vocab.speaker2id:\n",
        "                speaker_id = vocab.speaker2id[speaker]\n",
        "            else:\n",
        "                speaker_id = len(vocab.speaker2id) # val or test set, use a new speaker id\n",
        "            emotion_id = vocab.emotion2id[emotion]\n",
        "            ex_ids.append((utter_ids, speaker_id, emotion_id, mask))\n",
        "        examples_ids.append(ex_ids)\n",
        "    return examples_ids\n",
        "\n",
        "def create_one_batch(examples):\n",
        "    \"\"\"\n",
        "        examples: a batch of examples having the same number of turns and seq_len, each example is a list of (utter, speaker, label, mask)\n",
        "        \n",
        "        return: batch_x, batch_y, where batch_x is a tuple of token ids and speaker ids\n",
        "    \"\"\"\n",
        "    tokens = []\n",
        "    speakers = []\n",
        "    labels = []\n",
        "    masks = []\n",
        "    for ex in examples:\n",
        "        ex_tokens, ex_speakers, ex_labels, ex_masks = list(zip(*ex))\n",
        "        tokens.append(ex_tokens)\n",
        "        speakers.append(ex_speakers)\n",
        "        labels.append(ex_labels)\n",
        "        masks.append(ex_masks)\n",
        "    \n",
        "    return (tokens, speakers), labels, masks\n",
        "\n",
        "\n",
        "def create_batches(examples, batch_size, train=True):\n",
        "    batch_data = []\n",
        "    if train == True:\n",
        "        random.shuffle(examples)\n",
        "    \n",
        "    batch_ids = list(range(0, len(examples), batch_size)) + [len(examples)]\n",
        "    for s, e in zip(batch_ids[:-1], batch_ids[1:]):\n",
        "        batch_examples = examples[s:e]\n",
        "        batch_x, batch_y, batch_mask = create_one_batch(batch_examples)\n",
        "        batch_data.append((batch_x, batch_y, batch_mask))\n",
        "    return batch_data\n",
        "\n",
        "\n",
        "def create_balanced_batches(examples, batch_size, train=True):\n",
        "    batch_data = []\n",
        "    if train == True:\n",
        "        random.shuffle(examples)\n",
        "    \n",
        "    num_valid_utterances = []\n",
        "    for ex in examples:\n",
        "        num_valid_utterances.append(sum([mask for utter, speaker, label, mask in ex]))\n",
        "    \n",
        "    batch_ids = [0]\n",
        "    for i in range(len(examples)):\n",
        "        if len(num_valid_utterances[batch_ids[-1]:i]) >= batch_size:\n",
        "            batch_ids.append(i)\n",
        "    \n",
        "    for s, e in zip(batch_ids[:-1], batch_ids[1:]):\n",
        "        batch_examples = examples[s:e]\n",
        "        batch_x, batch_y, batch_mask = create_one_batch(batch_examples)\n",
        "        batch_data.append((batch_x, batch_y, batch_mask))\n",
        "    return batch_data\n",
        "\n",
        "\n",
        "def merge_splits(train, val):\n",
        "    if len(train[0]) > len(val[0]):\n",
        "        num_additional_utterances = len(train[0]) - len(val[0])\n",
        "        for ex in val:\n",
        "            if ex[-1][0] == ['this', 'is', 'a', 'dummy', 'sentence']:\n",
        "                dummy_ex = ex[-1]\n",
        "                break\n",
        "        new_val = []\n",
        "        for ex in val:\n",
        "            new_val.append(ex + num_additional_utterances*[dummy_ex])\n",
        "        return train + new_val\n",
        "    elif len(train[0]) < len(val[0]):\n",
        "        num_additional_utterances = len(val[0]) - len(train[0])\n",
        "        for ex in train:\n",
        "            if ex[-1][0] == ['this', 'is', 'a', 'dummy', 'sentence']:\n",
        "                dummy_ex = ex[-1]\n",
        "                break\n",
        "        new_train = []\n",
        "        for ex in train:\n",
        "            new_train.append(ex + num_additional_utterances*[dummy_ex])\n",
        "        return new_train + val\n",
        "    else:\n",
        "        return train + val\n",
        "\n",
        "# conceptnet\n",
        "def get_vocab_embedding(vocab, vectors, embedding_size):\n",
        "    pretrained_word_embedding = np.zeros((len(vocab.word2id), embedding_size))\n",
        "    for w, i in vocab.word2id.items():\n",
        "        pretrained_word_embedding[i] = vectors.query(w)\n",
        "    return pretrained_word_embedding\n",
        "\n",
        "\n",
        "def get_emotion_intensity(NRC, word):\n",
        "    if word not in NRC:\n",
        "        word = porter.stem(word)\n",
        "        if word not in NRC:\n",
        "            return 0.5\n",
        "    v, a, d = NRC[word]\n",
        "    a = a/2\n",
        "    return (np.linalg.norm(np.array([v, a]) - np.array([0.5, 0])) - 0.06467)/0.607468\n",
        "\n",
        "\n",
        "# edge matrix construction\n",
        "def filter_conceptnet(conceptnet, vocab):\n",
        "    filtered_conceptnet = {}\n",
        "    for k in conceptnet:\n",
        "        if k in vocab.word2id and k not in stopwords:\n",
        "            filtered_conceptnet[k] = set()\n",
        "            for c,w in conceptnet[k]:\n",
        "                if c in vocab.word2id and c not in stopwords and w>=1:\n",
        "                    filtered_conceptnet[k].add((c,w))\n",
        "    return filtered_conceptnet\n",
        "\n",
        "\n",
        "# remove cases where the same concept has multiple weights\n",
        "def remove_KB_duplicates(conceptnet):\n",
        "    filtered_conceptnet = {}\n",
        "    for k in conceptnet:\n",
        "        filtered_conceptnet[k] = set()\n",
        "        concepts = set()\n",
        "        filtered_concepts = sorted(conceptnet[k], key=lambda x: x[1], reverse=True)\n",
        "        for c,w in filtered_concepts:\n",
        "            if c not in concepts:\n",
        "                filtered_conceptnet[k].add((c, w))\n",
        "                concepts.add(c)\n",
        "    return filtered_conceptnet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-TxtLvmtnQN"
      },
      "source": [
        "# Prepare tools"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cqjPf7Vqhqt"
      },
      "source": [
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "def counter_to_distribution(counter):\n",
        "    distribution = {}\n",
        "    total = sum([v for k, v in counter.items()])\n",
        "    for k, v in counter.items():\n",
        "        distribution[k] = round(v/total, 4)\n",
        "    return distribution\n",
        "\n",
        "\n",
        "def label_distribution_transformer(examples):\n",
        "    counter = Counter([ex[-1] for ex in examples])\n",
        "    percentatges = []\n",
        "    for i in range(len(counter)):\n",
        "        percentatges.append(counter[i]/len(examples))\n",
        "    return percentatges"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYZpuJsQt216"
      },
      "source": [
        "# Prepare model transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2v_0nNTSwdu9"
      },
      "source": [
        "embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EcEvDi-q9Db"
      },
      "source": [
        "#embedding\n",
        "print_dims = False\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "class Embeddings(nn.Module):\n",
        "    def __init__(self, d_model, vocab):\n",
        "        super(Embeddings, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab, d_model)\n",
        "        self.d_model = d_model\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "            x: (batch_size, seq_len)\n",
        "        \"\"\"\n",
        "        if print_dims:\n",
        "            print(\"{0}: x: type: {1}, shape: {2}\".format(self.__class__.__name__, x.type(), x.shape))\n",
        "        return self.embedding(x) * math.sqrt(self.d_model)\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        \n",
        "        # compute the positional encodings once in log space\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0.0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0.0, d_model, 2) * -(math.log(10000.0)/d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "            x: (batch_size, seq_len, d_model)\n",
        "        \"\"\"\n",
        "        if print_dims:\n",
        "            print(\"{0}: x: type: {1}, shape: {2}\".format(self.__class__.__name__, x.type(), x.shape))\n",
        "        x = x + self.pe[:, :x.size(1)]\n",
        "        return self.dropout(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7fIX4VKwf81"
      },
      "source": [
        "module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0IpwE-uwXzT"
      },
      "source": [
        "#import module (inside attention)\n",
        "import copy\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "print_dims = False\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    \"Construct a layernorm module\"\n",
        "    def __init__(self, n_features, eps=1e-6):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.a_2 = nn.Parameter(torch.ones(n_features))\n",
        "        self.b_2 = nn.Parameter(torch.zeros(n_features))\n",
        "        self.eps = eps\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "            x: (batch_size, seq_len, d_model)\n",
        "        \"\"\"\n",
        "        if print_dims:\n",
        "            print(\"{0}: x: type: {1}, shape: {2}\".format(self.__class__.__name__, x.type(), x.shape))\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        std = x.std(dim=-1, keepdim=True)\n",
        "        return self.a_2 * (x - mean)/(std + self.eps) + self.b_2\n",
        "\n",
        "class SublayerConnection(nn.Module):\n",
        "    \"\"\"\n",
        "    A residual connection followed by a layer norm.\n",
        "    The norm is applied first as apposed to last\n",
        "    \"\"\"\n",
        "    def __init__(self, size, dropout):\n",
        "        super(SublayerConnection, self).__init__()\n",
        "        self.norm = LayerNorm(size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x, sublayer):\n",
        "        \"\"\"\n",
        "            x: (batch_size, seq_len, d_model)\n",
        "        \"\"\"\n",
        "        if print_dims:\n",
        "            print(\"{0}: x: type: {1}, shape: {2}\".format(self.__class__.__name__, x.type(), x.shape))\n",
        "        \"Apply residual connection to any sublayer with the same size\"\n",
        "        return x + self.dropout(sublayer(self.norm(x)))\n",
        "\n",
        "class PositionWiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super(PositionWiseFeedForward, self).__init__()\n",
        "        self.w1 = nn.Linear(d_model, d_ff)\n",
        "        self.w2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "            x: (batch_size, seq_len, d_model)\n",
        "        \"\"\"\n",
        "        if print_dims:\n",
        "            print(\"{0}: x: type: {1}, shape: {2}\".format(self.__class__.__name__, x.type(), x.shape))\n",
        "        return self.w2(self.dropout(F.relu(self.w1(x))))\n",
        "\n",
        "\n",
        "def clones(module, N):\n",
        "    \"\"\"\n",
        "    Produce N identical layers\n",
        "    \"\"\"\n",
        "    return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-3GQI2QwjS_"
      },
      "source": [
        "attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bF47oC7BtGd5"
      },
      "source": [
        "# including graph attention (proposed in this project)\n",
        "\n",
        "def attention(query, key, value, mask=None, dropout=None):\n",
        "    \"\"\"\n",
        "    Compute scaled dot product self attention\n",
        "        query: (batch_size, h, seq_len, d_k), seq_len can be either src_seq_len or tgt_seq_len\n",
        "        key: (batch_size, h, seq_len, d_k), seq_len in key, value and mask are the same\n",
        "        value: (batch_size, h, seq_len, d_k)\n",
        "        mask: (batch_size, 1, 1, seq_len) or (batch_size, 1, tgt_seq_len, tgt_seq_len) (legacy)\n",
        "    \"\"\"\n",
        "    if print_dims:\n",
        "        print(\"{0}: query: type: {1}, shape: {2}\".format(\"attention func\", query.type(), query.shape))\n",
        "        print(\"{0}: key: type: {1}, shape: {2}\".format(\"attention func\", key.type(), key.shape))\n",
        "        print(\"{0}: value: type: {1}, shape: {2}\".format(\"attention func\", value.type(), value.shape))\n",
        "        print(\"{0}: mask: type: {1}, shape: {2}\".format(\"attention func\", mask.type(), mask.shape))\n",
        "    d_k = query.size(-1)\n",
        "\n",
        "    # scores: (batch_size, h, seq_len, seq_len) for self_attn, (batch_size, h, tgt_seq_len, src_seq_len) for src_attn\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1)/math.sqrt(d_k)) \n",
        "    # print(query.shape, key.shape, mask.shape, scores.shape)\n",
        "\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "    p_attn = F.softmax(scores, dim=-1)\n",
        "    if dropout is not None:\n",
        "        p_attn = dropout(p_attn)\n",
        "    return torch.matmul(p_attn, value), p_attn\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, h, d_model, dropout=0.1):\n",
        "        print(\"Nilai H :\", h)\n",
        "        print(\"Nilai d_model :\", d_model)\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        assert d_model % h == 0\n",
        "        self.d_k = d_model//h\n",
        "        self.h = h\n",
        "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
        "        self.attn = None\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.a = nn.Parameter(torch.Tensor([1]))\n",
        "        self.b = nn.Parameter(torch.Tensor([1]))\n",
        "        \n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        \"\"\"\n",
        "            query: (batch_size, seq_len, d_model), seq_len can be either src_seq_len or tgt_seq_len\n",
        "            key: (batch_size, seq_len, d_model), seq_len in key, value and mask are the same\n",
        "            value: (batch_size, seq_len, d_model)\n",
        "            mask: (batch_size, 1, seq_len) or (batch_size, tgt_seq_len, tgt_seq_len) (legacy)\n",
        "        \"\"\"\n",
        "        if print_dims:\n",
        "            print(\"{0}: query: type: {1}, shape: {2}\".format(self.__class__.__name__, query.type(), query.shape))\n",
        "            print(\"{0}: key: type: {1}, shape: {2}\".format(self.__class__.__name__, key.type(), key.shape))\n",
        "            print(\"{0}: value: type: {1}, shape: {2}\".format(self.__class__.__name__, value.type(), value.shape))\n",
        "            print(\"{0}: mask: type: {1}, shape: {2}\".format(self.__class__.__name__, mask.type(), mask.shape))\n",
        "        if mask is not None:\n",
        "            mask = mask.unsqueeze(1)\n",
        "        nbatches = query.size(0)\n",
        "        \n",
        "        # 1) Do all linear projections in batch from d_model to (h, d_k)\n",
        "        query, key, value = [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1,2)\n",
        "            for l, x in zip(self.linears, (query, key, value))]\n",
        "        \n",
        "        # 2) Apply attention on all the projected vectors in batch\n",
        "        x, self.attn = attention(query, key, value, mask=mask, dropout=self.dropout) # (batch_size, h, seq_len, d_k)\n",
        "        if print_dims:\n",
        "            print(\"{0}: x (after attention): type: {1}, shape: {2}\".format(self.__class__.__name__, x.type(), x.shape))\n",
        "\n",
        "        # 3) Concatenate and apply a final linear\n",
        "        x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k)\n",
        "        x = self.linears[-1](x) # (batch_size, seq_len, d_model)\n",
        "        if print_dims:\n",
        "            print(\"{0}: x (after concatenation and linear): type: {1}, shape: {2}\".format(self.__class__.__name__, x.type(), x.shape))\n",
        "        return x\n",
        "\n",
        "\n",
        "class GraphAttention(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, GAW=None, variant=0, concept_embed=None):\n",
        "        super(GraphAttention, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_size = embed_size\n",
        "        self.variant = variant\n",
        "        if concept_embed is None:\n",
        "            self.concept_embed = nn.Embedding(vocab_size, embed_size)\n",
        "        else:\n",
        "            self.concept_embed = concept_embed\n",
        "\n",
        "    def init_params(self, GAW=None, edge_matrix=None, affectiveness=None, concentration_factor=1):\n",
        "        self.GAW = GAW\n",
        "        edge_matrix_range = (edge_matrix.max(dim=1)[0] - edge_matrix.min(dim=0)[0]).unsqueeze(1)\n",
        "        edge_matrix = edge_matrix/(edge_matrix_range + (edge_matrix_range==0).float()) # normalization\n",
        "        self.edge_matrix = edge_matrix\n",
        "        self.affectiveness = affectiveness\n",
        "        self.concentration_factor = concentration_factor\n",
        "        if self.GAW is not None:\n",
        "            self._lambda = self.GAW\n",
        "        else:\n",
        "            self._lambda = nn.Parameter(torch.full((self.vocab_size,), 0.5))\n",
        "\n",
        "\n",
        "    def get_hierarchical_sentence_representation(self, sent_embed):\n",
        "        # sent_embed: (batch_size, seq_len, d_model)\n",
        "        seq_len = sent_embed.shape[1]\n",
        "        N = 3 # n-gram hierarchical pooling\n",
        "        \n",
        "        # max pooling for each ngram\n",
        "        ngram_embeddings = [[] for i in range(N-1)] # one list for each n\n",
        "        for n in range(1, N):\n",
        "            for i in range(seq_len):\n",
        "                ngram_embeddings[n-1].append(sent_embed[:,i:i+n+1,:].max(dim=1)[0])\n",
        "        \n",
        "        # mean pooling across ngram embeddings\n",
        "        pooled_ngram_embeddings = [sent_embed.mean(dim=1)] # unigram\n",
        "        for ngram_embedding in ngram_embeddings:\n",
        "            ngram_embedding = torch.stack(ngram_embedding, dim=1).mean(dim=1)\n",
        "            pooled_ngram_embeddings.append(ngram_embedding)\n",
        "\n",
        "        sent_embed = torch.stack(pooled_ngram_embeddings, dim=1).mean(dim=1)\n",
        "        return sent_embed\n",
        "\n",
        "    def get_context_representation(self, src_embed, tgt_embed):\n",
        "        # src_embed: (batch_size, context_length*seq_len, d_model)\n",
        "        # tgt_embed: (batch_size, seq_len, d_model)\n",
        "        seq_len = tgt_embed.shape[1]\n",
        "        context_length = src_embed.shape[1]//seq_len\n",
        "        sentence_representations = []\n",
        "        for i in range(context_length):\n",
        "            sentence_representations.append(self.get_hierarchical_sentence_representation(src_embed[:, i*seq_len:(i+1)*seq_len]))\n",
        "        sentence_representations.append(self.get_hierarchical_sentence_representation(tgt_embed))\n",
        "        context_representation = torch.stack(sentence_representations, dim=1).mean(dim=1) # (batch_size, d_model)\n",
        "        return context_representation\n",
        "\n",
        "    def forward(self, src, src_embed, tgt, tgt_embed):\n",
        "        # src: (batch_size, context_length * seq_len)\n",
        "        # src_embed: (batch_size, context_length*seq_len, d_model)\n",
        "        # tgt: (batch_size, seq_len)\n",
        "        # tgt_embed: (batch_size, seq_len, d_model)\n",
        "        # embed: shared embedding layer: (vocab_size, d_model)\n",
        "\n",
        "        # get context representation\n",
        "        if self.variant == 1:\n",
        "            # standard attention as in GAT\n",
        "            src_len = src.shape[1]\n",
        "            src = torch.cat([src, tgt], dim=1)\n",
        "            src_embed = torch.cat([src_embed, tgt_embed], dim=1) # (batch_size, (context_length+1)*seq_len, d_model), self.concept_embed.weight: (vocab_size, d_model)\n",
        "            concept_weights = (self.edge_matrix[src] > 0).float() * torch.matmul(src_embed, self.concept_embed.weight.transpose(0,1)) # (batch_size, (context_length+1)*seq_len, vocab_size)\n",
        "            concept_embedding = torch.matmul(torch.softmax(concept_weights * self.concentration_factor, dim=2), self.concept_embed.weight)\n",
        "            return concept_embedding[:, :src_len, :], concept_embedding[:, src_len:, :]\n",
        "        if self.variant == 2:\n",
        "            context_representation = self.get_context_representation(src_embed, tgt_embed) # (batch_size, d_model)\n",
        "            # get concept embedding\n",
        "            src_len = src.shape[1]\n",
        "            src = torch.cat([src, tgt], dim=1)\n",
        "            cosine_similarity = torch.abs(torch.cosine_similarity(context_representation.unsqueeze(1), \\\n",
        "                self.concept_embed.weight.unsqueeze(0), dim=2)) # (batch_size, vocab_size)\n",
        "            cosine_similarity = cosine_similarity.to(device)\n",
        "            relatedness = self.edge_matrix[src] * cosine_similarity.unsqueeze(1) # (batch_size, (context_length+1)*seq_len, vocab_size)\n",
        "            concept_weights = self._lambda*relatedness + (1-self._lambda)*(self.edge_matrix[src] > 0).float()*self.affectiveness # (batch_size, (context_length+1)*seq_len, vocab_size)\n",
        "            concept_embedding = torch.matmul(torch.softmax(concept_weights * self.concentration_factor, dim=2), self.concept_embed.weight) # (batch_size, (context_length+1)*seq_len, d_model)\n",
        "            return concept_embedding[:, :src_len, :], concept_embedding[:, src_len:, :]\n",
        "            print(\"cek cosine\")\n",
        "            cosine_similarity.is_cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4We5y20_wrrC"
      },
      "source": [
        "encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cizo7yfNv1hF"
      },
      "source": [
        "#encoder\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "print_dims = False\n",
        "class Encoder(nn.Module):\n",
        "    \"Encoder is a stack of N layers\"\n",
        "    def __init__(self, layer, N):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.layers = clones(layer, N)\n",
        "        self.norm = LayerNorm(layer.size)\n",
        "        \n",
        "    def forward(self, x, mask):\n",
        "        \"\"\"\n",
        "            x: (batch_size, src_seq_len, d_model)\n",
        "            mask: (batch_size, 1, src_seq_len)\n",
        "        \"\"\"\n",
        "        \"Pass the input token ids and mask through each layer in turn\"\n",
        "        if print_dims:\n",
        "            print(\"{0}: x: type: {1}, shape: {2}\".format(self.__class__.__name__, x.type(), x.shape))\n",
        "            print(\"{0}: mask: type: {1}, shape: {2}\".format(self.__class__.__name__, mask.type(), mask.shape))\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    \"Encoder is made up of self-attn and feed forward layers\"\n",
        "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = self_attn\n",
        "        self.feed_forward = feed_forward\n",
        "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
        "        self.size = size\n",
        "        \n",
        "    def forward(self, x, mask):\n",
        "        \"\"\"norm -> self_attn -> dropout -> add -> \n",
        "        norm -> feed_forward -> dropout -> add\"\"\"\n",
        "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
        "        return self.sublayer[1](x, self.feed_forward)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1sYoV2Fw0Kt"
      },
      "source": [
        "decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQcxLfRBw2DW"
      },
      "source": [
        "#decoder\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "print_dims = False\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    \"Generic N layer decoder with masking\"\n",
        "    def __init__(self, layer, N):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.layers = clones(layer, N)\n",
        "        self.norm = LayerNorm(layer.size)\n",
        "        \n",
        "    def forward(self, x, src_states, src_mask, tgt_mask):\n",
        "        \"\"\"\n",
        "            x: (batch_size, tgt_seq_len, d_model)\n",
        "            src_states: (batch_size, src_seq_len, d_model)\n",
        "            src_mask: (batch_size, 1, src_seq_len)\n",
        "            tgt_mask: (batch_size, tgt_seq_len, tgt_seq_len)\n",
        "        \"\"\"\n",
        "        if print_dims:\n",
        "            print(\"{0}: x: type: {1}, shape: {2}\".format(self.__class__.__name__, x.type(), x.shape))\n",
        "            print(\"{0}: src_states: type: {1}, shape: {2}\".format(self.__class__.__name__, src_states.type(), src_states.shape))\n",
        "            print(\"{0}: src_mask: type: {1}, shape: {2}\".format(self.__class__.__name__, src_mask.type(), src_mask.shape))\n",
        "            print(\"{0}: tgt_mask: type: {1}, shape: {2}\".format(self.__class__.__name__, tgt_mask.type(), tgt_mask.shape))\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, src_states, src_mask, tgt_mask)\n",
        "        x = self.norm(x) # (batch_size, tgt_seq_len, d_model)\n",
        "        if print_dims:\n",
        "            print(\"{0}: x (output): type: {1}, shape: {2}\".format(self.__class__.__name__, x.type(), x.shape))\n",
        "        \n",
        "        # add max pooling across sequences\n",
        "        x = F.max_pool1d(x.permute(0,2,1), x.shape[1]).squeeze(-1) # (batch_size, d_model)\n",
        "        return x\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    \"Decoder is made of self-attn, src-attn and feed forward\"\n",
        "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.size = size\n",
        "        self.self_attn = self_attn\n",
        "        self.src_attn = src_attn\n",
        "        self.feed_forward = feed_forward\n",
        "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
        "        \n",
        "    def forward(self, x, src_states, src_mask, tgt_mask):\n",
        "        \"\"\"norm -> self_attn -> dropout -> add -> \n",
        "        norm -> src_attn -> dropout -> add ->\n",
        "        norm -> feed_forward -> dropout -> add\"\"\"\n",
        "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
        "        x = self.sublayer[1](x, lambda x: self.src_attn(x, src_states, src_states, src_mask))\n",
        "        return self.sublayer[2](x, self.feed_forward)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmyYWyYZxFDt"
      },
      "source": [
        "generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTe6XtaqxGhG"
      },
      "source": [
        "#generator\n",
        "class Generator(nn.Module):\n",
        "    \"\"\"\n",
        "    A standard linear + softmax generation step\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, vocab):\n",
        "        super(Generator, self).__init__()\n",
        "        self.proj = nn.Linear(d_model, vocab)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        if print_dims:\n",
        "            print(\"{0}: x: type: {1}, shape: {2}\".format(self.__class__.__name__, x.type(), x.shape))\n",
        "        return F.log_softmax(self.proj(x), dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7H3fGdXxhLE"
      },
      "source": [
        "Encoder Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekcv2YBAxj50"
      },
      "source": [
        "class EncoderDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    A standard Encoder-Decoder architecture\n",
        "    \"\"\"\n",
        "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator, model_variant=1, graph_attention=None, concept_emb=None):\n",
        "        super(EncoderDecoder, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_embed = src_embed\n",
        "        self.tgt_embed = tgt_embed\n",
        "        self.generator = generator\n",
        "        self.model_variant = model_variant\n",
        "        self.graph_attention = graph_attention\n",
        "        self.concept_emb = concept_emb\n",
        "        \n",
        "        self.embed_size = self.src_embed[0].embedding.weight.shape[1]\n",
        "        self.src_mlp = nn.Linear(2*self.embed_size, self.embed_size)\n",
        "\n",
        "        \n",
        "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
        "        # src: (batch_size, context_length * seq_len)\n",
        "        # src_mask: (batch_size, 1, context_length * seq_len)\n",
        "        # tgt: (batch_size, seq_len)\n",
        "        # tgt_mask: (batch_size, 1, seq_len)\n",
        "        # concept: (batch_size, (context_length + 1) * 30)\n",
        "\n",
        "        src_embed = self.src_embed(src) # src_embed: (batch_size, context_length * seq_len, d_model)\n",
        "        tgt_embed = self.tgt_embed(tgt) # tgt_embed: (batch_size, seq_len, d_model)\n",
        "        if self.graph_attention is not None:\n",
        "            # graph attention to compute concept representations\n",
        "            src_concept_embed, tgt_concept_embed = self.graph_attention(src, src_embed, tgt, tgt_embed)\n",
        "\n",
        "            # concatenate concept representations with utterance embeddings\n",
        "            src_embed = self.src_mlp(torch.cat([src_embed, src_concept_embed], dim=-1))\n",
        "            tgt_embed = self.src_mlp(torch.cat([tgt_embed, tgt_concept_embed], dim=-1))\n",
        "        \n",
        "        if self.model_variant == 1:\n",
        "            # self-attention\n",
        "            src_states = self.encoder(src_embed, src_mask)\n",
        "            # cross-attention\n",
        "            return self.decoder(tgt_embed, src_states, src_mask, tgt_mask)\n",
        "        if self.model_variant == 2:\n",
        "            seq_len = tgt.shape[1]\n",
        "            context_length = src.shape[1]//seq_len\n",
        "            \n",
        "            # utterance-level self-attention for each src sentence\n",
        "            src_states = []\n",
        "            for i in range(context_length):\n",
        "                s, e = i*seq_len, (i+1)*seq_len\n",
        "                src_states.append(self.encoder(src_embed[:,s:e,:], src_mask[:,:,s:e]))\n",
        "            src_states = torch.cat(src_states, dim=1)\n",
        "            \n",
        "            # context-level self-attention\n",
        "            src_states = self.encoder(src_states, src_mask)\n",
        "            \n",
        "            # cross-attention\n",
        "            return self.decoder(tgt_embed, src_states, src_mask, tgt_mask)\n",
        "\n",
        "\n",
        "def make_model(src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, output_size=0, dropout=0.1, KB=False, \\\n",
        "        model_variant=1, context_length=0, graph_attention_variant=0):\n",
        "    c = copy.deepcopy\n",
        "    enc_attn = MultiHeadAttention(h, d_model)\n",
        "    dec_attn = MultiHeadAttention(h, d_model)\n",
        "    enc_dec_attn = MultiHeadAttention(h, d_model)\n",
        "    ff = PositionWiseFeedForward(d_model, d_ff, dropout)\n",
        "    position = PositionalEncoding(d_model, dropout)\n",
        "    emb = Embeddings(d_model, src_vocab) # share src and tgt embedding\n",
        "    concept_embed = None\n",
        "    graph_attention = None\n",
        "    if KB:\n",
        "        # dynamic graph\n",
        "        graph_attention = GraphAttention(vocab_size=src_vocab, embed_size=d_model, variant=graph_attention_variant, concept_embed=concept_embed)\n",
        "    \n",
        "    model = EncoderDecoder(\n",
        "        Encoder(EncoderLayer(d_model, c(enc_attn), c(ff), dropout), N),\n",
        "        Decoder(DecoderLayer(d_model, c(dec_attn), c(enc_dec_attn), c(ff), dropout), N),\n",
        "        nn.Sequential(emb, c(position)),\n",
        "        nn.Sequential(emb, c(position)),\n",
        "        Generator(d_model, output_size),\n",
        "        model_variant=model_variant,\n",
        "        graph_attention=graph_attention\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgOVnpm0x3nl"
      },
      "source": [
        "# Prepare batch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvrvOLe2x6ks"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# no tgt mask for classification\n",
        "def no_mask(size):\n",
        "    \"Mask out subsequent positions.\"\n",
        "    attn_shape = (1, size, size) # size: seq_len\n",
        "    subsequent_mask = np.zeros(attn_shape).astype('uint8')\n",
        "    # subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
        "    # print(\"subsequent mask shape: \", subsequent_mask.shape) # (1, seq_len-1, seq_len-1)\n",
        "    return torch.from_numpy(subsequent_mask) == 0\n",
        "\n",
        "\n",
        "class ClassificationBatch:\n",
        "    \"Object for holding a batch of data with mask during training\"\n",
        "    def __init__(self, src, tgt, label, pad=0, concept=None):\n",
        "        # print(\"src, tgt shape:\", src.shape, tgt.shape)\n",
        "        self.src = src\n",
        "        self.src_mask = (src != pad).unsqueeze(-2)\n",
        "        self.tgt = tgt\n",
        "        self.concept = concept\n",
        "        self.concept_mask = None\n",
        "        if concept is not None:\n",
        "            self.concept_mask = (concept != pad).unsqueeze(-2)\n",
        "        self.tgt_mask = (tgt != pad).unsqueeze(-2)\n",
        "        self.tgt_y = label\n",
        "        # self.tgt_mask = self.make_std_mask(self.tgt, pad)\n",
        "        self.ntokens = torch.FloatTensor([len(src)])[0] # batch_size\n",
        "        \n",
        "    def to(self, device):\n",
        "        self.src = self.src.to(device)\n",
        "        self.src_mask = self.src_mask.to(device)\n",
        "        self.tgt = self.tgt.to(device)\n",
        "        self.tgt_mask = self.tgt_mask.to(device)\n",
        "        if self.concept is not None:\n",
        "            self.concept = self.concept.to(device)\n",
        "            self.concept_mask = self.concept_mask.to(device)\n",
        "        self.tgt_y = self.tgt_y.to(device)\n",
        "        self.ntokens = self.ntokens.to(device)\n",
        "    \n",
        "    @staticmethod\n",
        "    def make_std_mask(tgt, pad):\n",
        "        \"Create a mask to hide padding and future words\"\n",
        "        tgt_mask = (tgt != pad).unsqueeze(-2) # (batch_size, 1, seq_len-1)\n",
        "        # print(\"tgt_mask shape: \", tgt_mask.shape)\n",
        "        tgt_mask = tgt_mask & no_mask(tgt.size(-1)).type_as(tgt_mask)\n",
        "        return tgt_mask\n",
        "\n",
        "\n",
        "def flatten_examples_classification(examples, vocab, k=1):\n",
        "    # returns a list of ([(utter1, speaker1), (utter2, speaker2), ..., (utterk, speakerk)], label)\n",
        "    import random\n",
        "    classfication_examples = []\n",
        "    all_speakers = list(vocab.speaker2id.values())\n",
        "    empty_ex = len(examples[0][0][0])*[vocab.word2id[\"<pad>\"]]\n",
        "    for ex in examples:\n",
        "        for i in range(len(ex)):\n",
        "            mask = ex[i][-1]\n",
        "            if mask == 1:\n",
        "                if i < k:\n",
        "                    context = [(empty_ex.copy(), random.choice(all_speakers)) for i in range(k-i)] # k-i\n",
        "                    context += [(ex[i-j][0].copy(), ex[i-j][1]) for j in range(i, 0, -1)] # (k-i) + (i) = k\n",
        "                else:\n",
        "                    context = [(ex[i-j][0].copy(), ex[i-j][1]) for j in range(k, 0, -1)] # k\n",
        "                if k > 0:\n",
        "                    new_ex = (context + [(ex[i][0].copy(), ex[i][1])], ex[i][2])\n",
        "                else:\n",
        "                    new_ex = [(empty_ex.copy(), random.choice(all_speakers)), (ex[i][0].copy(), ex[i][1])], ex[i][2] # add one empty context utterance\n",
        "                # if i==0:\n",
        "                #     random_speaker = random.choice(all_speakers)\n",
        "                #     new_ex = [(empty_ex.copy(), random_speaker), (ex[i][0].copy(), ex[i][1])], ex[i][2]\n",
        "                # else:\n",
        "                #     new_ex = [(ex[i-1][0].copy(), ex[i-1][1]), (ex[i][0].copy(), ex[i][1])], ex[i][2]\n",
        "                classfication_examples.append(new_ex)\n",
        "    return classfication_examples\n",
        "\n",
        "\n",
        "def create_batches_classification(examples, batch_size, vocab, train=True):\n",
        "    import random\n",
        "    \n",
        "    def create_one_batch(examples, vocab):\n",
        "        \"\"\"\n",
        "            examples: a batch of examples having the same number of turns and seq_len, \n",
        "                each example is a list of ([(utter1, speaker1), ..., (utterk, speakerk), (utterA, speakerA)], label)\n",
        "            return: batch_Q, batch_Q_speakers, batch_A, batch_A_speakers, batch_label\n",
        "        \"\"\"\n",
        "        Qs = [] # context utterances\n",
        "        Q_speakers = [] # context speakers\n",
        "        As = [] # current utterance\n",
        "        A_speakers = [] # current speaker\n",
        "        labels = [] # label of current utterance\n",
        "\n",
        "        for ex in examples:\n",
        "            context = []\n",
        "            context_speakers = []\n",
        "            for Q, Q_speaker in ex[0][:-1]:\n",
        "                context.extend(Q)\n",
        "                context_speakers.append(Q_speaker)\n",
        "            A, A_speaker = ex[0][-1]\n",
        "            label = ex[1]\n",
        "            \n",
        "            Qs.append(context)\n",
        "            Q_speakers.append(context_speakers)\n",
        "            As.append(A)\n",
        "            A_speakers.append(A_speaker)\n",
        "            labels.append(label)\n",
        "        \n",
        "        batch = ClassificationBatch(torch.LongTensor(Qs), torch.LongTensor(As), torch.LongTensor(labels), vocab.word2id[\"<pad>\"])\n",
        "        return batch\n",
        "\n",
        "    batch_data = []\n",
        "    if train == True:\n",
        "        random.shuffle(examples)\n",
        "    batch_ids = list(range(0, len(examples), batch_size)) + [len(examples)]\n",
        "    for s, e in zip(batch_ids[:-1], batch_ids[1:]):\n",
        "        batch_examples = examples[s:e]\n",
        "        one_batch = create_one_batch(batch_examples, vocab)\n",
        "        batch_data.append(one_batch)\n",
        "    return batch_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Keo3iuqXyJwy"
      },
      "source": [
        "# prepare loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRrn9MwvyMVd"
      },
      "source": [
        "#loss.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from sklearn.metrics import f1_score, mean_absolute_error, classification_report\n",
        "\n",
        "class NoamOpt:\n",
        "    def __init__(self, model_size, factor, warmup, optimizer):\n",
        "        self.optimizer = optimizer\n",
        "        self._step = 0\n",
        "        self.warmup = warmup\n",
        "        self.factor = factor\n",
        "        self.model_size = model_size\n",
        "        self._rate = 0\n",
        "    \n",
        "    def step(self):\n",
        "        \"Update parameter and rate\"\n",
        "        self._step += 1\n",
        "        rate = self.rate()\n",
        "        for p in self.optimizer.param_groups:\n",
        "            p[\"lr\"] = rate\n",
        "        self._rate = rate\n",
        "        self.optimizer.step()\n",
        "        \n",
        "    def rate(self, step=None):\n",
        "        if step is None:\n",
        "            step = self._step\n",
        "        return self.factor * (self.model_size ** (-0.5) * \n",
        "                              min(step ** (-0.5), step * self.warmup ** (-1.5)))\n",
        "\n",
        "def get_std_opt(model):\n",
        "    return NoamOpt(model.src_embed[0].d_model, 2, 4000, \n",
        "                  torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n",
        "\n",
        "class LabelSmoothing(nn.Module):\n",
        "    \"Implement label smoothing.\"\n",
        "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
        "        super(LabelSmoothing, self).__init__()\n",
        "        self.criterion = nn.KLDivLoss(reduction=\"sum\")\n",
        "        self.padding_idx = padding_idx\n",
        "        self.confidence = 1.0 - smoothing\n",
        "        self.smoothing = smoothing\n",
        "        self.size = size\n",
        "        self.true_dist = None\n",
        "        \n",
        "    def forward(self, x, target):\n",
        "        if print_dims:\n",
        "            print(\"{0}: x: type: {1}, shape: {2}\".format(self.__class__.__name__, x.type(), x.shape))\n",
        "            print(\"{0}: target: type: {1}, shape: {2}\".format(self.__class__.__name__, target.type(), target.shape))\n",
        "        assert x.size(1) == self.size\n",
        "        true_dist = x.data.clone()\n",
        "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
        "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
        "        true_dist[:, self.padding_idx] = 0\n",
        "        mask = torch.nonzero(target.data == self.padding_idx)\n",
        "        if mask.dim() > 0:\n",
        "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
        "        self.true_dist = true_dist\n",
        "        if print_dims:\n",
        "            print(\"{0}: true_dist: type: {1}, shape: {2}\".format(self.__class__.__name__, true_dist.type(), true_dist.shape))\n",
        "        return self.criterion(x, true_dist)\n",
        "\n",
        "\n",
        "class SimpleLossCompute:\n",
        "    \"A simple loss compute and train function.\"\n",
        "    def __init__(self, generator, criterion, dataset, emotion2id, opt=None, test=False):\n",
        "        self.generator = generator\n",
        "        self.criterion = criterion\n",
        "        self.opt = opt\n",
        "        self.dataset = dataset\n",
        "        self.emotion2id = emotion2id\n",
        "        self.test = test\n",
        "        self.outputs = []\n",
        "        self.tgts = []\n",
        "\n",
        "    @staticmethod\n",
        "    def compute_score(predictions, ground, dataset, emotion2id, test=False):\n",
        "        pred_y = predictions.astype(int)\n",
        "        val_y = ground.astype(int)\n",
        "        \n",
        "        if dataset in [\"EC\"]:\n",
        "            labels = [emotion2id[\"happy\"], emotion2id[\"angry\"], emotion2id[\"sad\"]]\n",
        "            score = f1_score(val_y, pred_y, average='micro', labels=labels)\n",
        "            print(\"Micro-F1 (exclude neutral): {0}\".format(score))\n",
        "            if test:\n",
        "                print(classification_report(val_y, pred_y, labels=labels, digits=4))\n",
        "        elif dataset in [\"DD\"]:\n",
        "            labels = [emotion2id[str(i)] for i in range(1, 7)]\n",
        "            score = f1_score(val_y, pred_y, average='micro', labels=labels)\n",
        "            print(\"Micro-F1 (exclude neutral): {0}\".format(score))\n",
        "            if test:\n",
        "                print(classification_report(val_y, pred_y, labels=labels, digits=4))\n",
        "        elif dataset in [\"MELD\", \"IEMOCAP\", \"EmoryNLP\"]:\n",
        "            score = f1_score(val_y, pred_y, average='weighted')\n",
        "            print(\"Weighted Macro-F1: {0}\".format(score))\n",
        "            if test:\n",
        "                print(classification_report(val_y, pred_y, digits=4))\n",
        "        else:\n",
        "            score = mean_absolute_error(val_y, pred_y)\n",
        "            print(\"MAE: {0}\".format(score))\n",
        "            if test:\n",
        "                print(mean_absolute_error(val_y, pred_y))\n",
        "        return score\n",
        "\n",
        "\n",
        "    def score(self):\n",
        "        score = self.compute_score(np.array(self.outputs), np.array(self.tgts), self.dataset, self.emotion2id, test=self.test)\n",
        "        return score\n",
        "\n",
        "\n",
        "    def clear(self):\n",
        "        self.outputs = []\n",
        "        self.tgts = []\n",
        "        \n",
        "    def __call__(self, x, y, norm):\n",
        "        \"\"\"\n",
        "        x: (batch_size, tgt_seq_len, d_model)\n",
        "        y: (batch_size, tgt_seq_len)\n",
        "        norm: ()\n",
        "        \"\"\"\n",
        "        if print_dims:\n",
        "            print(\"{0}: x: type: {1}, shape: {2}\".format(self.__class__.__name__, x.type(), x.shape))\n",
        "            print(\"{0}: y: type: {1}, shape: {2}\".format(self.__class__.__name__, y.type(), y.shape))\n",
        "            print(\"{0}: norm: type: {1}, shape: {2}\".format(self.__class__.__name__, norm.type(), norm.shape))\n",
        "        x = self.generator(x)\n",
        "        loss = self.criterion(x.contiguous().view(-1, x.size(-1)), y.contiguous().view(-1)) / norm\n",
        "        self.outputs += x.contiguous().view(-1, x.size(-1)).argmax(dim=-1).tolist()\n",
        "        self.tgts += y.contiguous().view(-1).tolist()\n",
        "        if print_dims:\n",
        "            print(\"{0}: loss: type: {1}, shape: {2}\".format(self.__class__.__name__, loss.type(), loss.shape))\n",
        "        \n",
        "        if self.opt is not None:\n",
        "            loss.backward()\n",
        "            self.opt.step()\n",
        "            if isinstance(self.opt, NoamOpt):\n",
        "                self.opt.optimizer.zero_grad()\n",
        "            else:\n",
        "                self.opt.zero_grad()\n",
        "        return loss.item() * norm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KJy_xLJyjYu"
      },
      "source": [
        "# Body program train.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRMtmv9NyoHM"
      },
      "source": [
        "logging.basicConfig(level = logging.INFO, \\\n",
        "                    format = '%(asctime)s  %(levelname)-5s %(message)s', \\\n",
        "                    datefmt =  \"%Y-%m-%d-%H-%M-%S\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H50nycul5CXr"
      },
      "source": [
        "device = torch.device(0)\n",
        "test_mode = \"True\" # False if for test data\n",
        "dataset = 'DD'\n",
        "min_freq = 1\n",
        "max_vocab_size = int(1e9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awRDNnA05o6Q"
      },
      "source": [
        "#this is hyperparameter\n",
        "model_variant = 2\n",
        "KB = \"conceptnet\"\n",
        "KB_percentage = 1.0\n",
        "graph_attention_variant = 2\n",
        "GAW = -1\n",
        "concentration_factor = 1\n",
        "context_length = 6 \n",
        "n_layers = 1\n",
        "d_model = 300\n",
        "d_ff = 400\n",
        "h = 4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAip66uS6Wlm"
      },
      "source": [
        "if context_length == 0:\n",
        "   KB = \"\"\n",
        "embedding_size = d_model\n",
        "    \n",
        "if dataset == \"EC\":\n",
        "   context_length = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alzcOstM6mYU"
      },
      "source": [
        "# training\n",
        "epochs = 10\n",
        "batch_size = 16 #semula:64\n",
        "lr = 4\n",
        "dropout = 0\n",
        "seed = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jM7O2OJI65eL"
      },
      "source": [
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tv2A5Qaw6_JI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3adbcbcf-a027-4c51-8551-0fdffa2a640a"
      },
      "source": [
        "# load examples\n",
        "import pickle\n",
        "\n",
        "logging.info(\"Loading data...\")\n",
        "train = load_pickle(\"/content/drive/MyDrive/Proyek Mandiri/data/DD/train.pkl\".format(dataset))\n",
        "val = load_pickle(\"/content/drive/MyDrive/Proyek Mandiri/data/DD/val.pkl\".format(dataset))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-06-19-08-57-39  INFO  Loading data...\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNGlIULgfTyw"
      },
      "source": [
        "if test_mode:\n",
        "   test = load_pickle(\"/content/drive/MyDrive/Proyek Mandiri/data/{0}/test.pkl\".format(dataset))\n",
        "   train = merge_splits(train, val)\n",
        "   val = test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irZesav97z3W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39d8f62c-593f-4e23-acf7-1a34b646ff71"
      },
      "source": [
        "logging.info(\"Number of training examples: {0}\".format(len(train)))\n",
        "logging.info(\"Number of validation examples: {0}\".format(len(val)))\n",
        "print(\"Number of training examples: {0}\".format(len(train)))\n",
        "print(\"Number of validation examples: {0}\".format(len(val)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-06-19-08-57-43  INFO  Number of training examples: 11118\n",
            "2021-06-19-08-57-43  INFO  Number of validation examples: 1000\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of training examples: 11118\n",
            "Number of validation examples: 1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8k7jhUyq8N-t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14e94629-7857-472e-b913-19f307cf5b11"
      },
      "source": [
        "for ex in train[0][:3]:\n",
        "   logging.info(\"Examples: {0}\".format(ex))\n",
        "logging.info(\"Building vocab...\")\n",
        "print(\"Building vocab...\")\n",
        "vocab = Vocab(train, min_freq, max_vocab_size)\n",
        "vocab_size = len(vocab.word2id)\n",
        "logging.info(\"Vocab size: {0}\".format(vocab_size))\n",
        "print(\"Vocab size: {0}\".format(vocab_size))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-06-19-08-58-55  INFO  Examples: (['say', ',', 'jim', ',', 'how', 'about', 'going', 'for', 'a', 'few', 'beers', 'after', 'dinner', '?'], 'Speaker A', '0', 1)\n",
            "2021-06-19-08-58-55  INFO  Examples: (['you', 'know', 'that', 'is', 'tempting', 'but', 'is', 'really', 'not', 'good', 'for', 'our', 'fitness', '.'], 'Speaker B', '0', 1)\n",
            "2021-06-19-08-58-55  INFO  Examples: (['what', 'do', 'you', 'mean', '?', 'it', 'will', 'help', 'us', 'to', 'relax', '.'], 'Speaker A', '0', 1)\n",
            "2021-06-19-08-58-55  INFO  Building vocab...\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Building vocab...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-06-19-08-58-56  INFO  Vocab size: 19447\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Vocab size: 19447\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzszEaOoU_05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16982fd4-8367-42ce-b23c-2dfb42bb4280"
      },
      "source": [
        "# build vocab and data    \n",
        "# use pretrained word embedding\n",
        "logging.info(\"Loading word embedding from Magnitude...\")\n",
        "home = os.path.expanduser(\"~\")\n",
        "if embedding_size in [50, 100, 200]:\n",
        "   vectors = Magnitude(\"http://magnitude.plasticity.ai/glove/light/glove.twitter.27B.{0}d.magnitude\".format(embedding_size))\n",
        "elif embedding_size in [300]:\n",
        "    # vectors = Magnitude(os.path.join(home, \"WordEmbedding/GoogleNews-vectors-negative{0}.magnitude\".format(embedding_size)))\n",
        "    vectors = Magnitude(\"http://magnitude.plasticity.ai/glove/medium/glove.840B.300d.magnitude\")\n",
        "pretrained_word_embedding = get_vocab_embedding(vocab, vectors, embedding_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-06-19-08-59-01  INFO  Loading word embedding from Magnitude...\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bhz5Qr6tVJk3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bf68a6e-b207-485f-8655-29f67e7fe417"
      },
      "source": [
        "if KB == \"conceptnet\":\n",
        "        # Calculate edge matrix\n",
        "        conceptnet = load_pickle(\"/content/drive/MyDrive/Proyek Mandiri/data/KB/DD.pkl\".format(dataset))\n",
        "        filtered_conceptnet = filter_conceptnet(conceptnet, vocab)\n",
        "        filtered_conceptnet = remove_KB_duplicates(filtered_conceptnet)\n",
        "        vocab_size = len(vocab.word2id)\n",
        "\n",
        "        edge_matrix = np.zeros((vocab_size, vocab_size))\n",
        "        #edge_matrix = torch.zeros([vocab_size, vocab_size], dtype=torch.float)\n",
        "        for k in filtered_conceptnet:\n",
        "            for c,w in filtered_conceptnet[k]:\n",
        "                edge_matrix[vocab.word2id[k], vocab.word2id[c]] = w\n",
        "        \n",
        "        # reduce size of KB\n",
        "        if KB_percentage > 0:\n",
        "            logging.info(\"Keeping {0}% KB concepts...\".format(KB_percentage*100))\n",
        "            edge_matrix = edge_matrix * (np.random.random((vocab_size,vocab_size)) < KB_percentage).astype(float)\n",
        "        logging.info(\"Edge matrix process completed\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-06-19-09-02-38  INFO  Keeping 100.0% KB concepts...\n",
            "2021-06-19-09-02-48  INFO  Edge matrix process completed\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQv8xw4QkdBg"
      },
      "source": [
        "#creating tensorflow from edgematix\n",
        "if KB == \"conceptnet\":\n",
        "  edge_matrix = torch.FloatTensor(edge_matrix).to(device)\n",
        "  #edge_matrix = edge_matrix.to(device) \n",
        "  edge_matrix[torch.arange(vocab_size), torch.arange(vocab_size)] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bQUGAX57zEQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a60170df-e49d-4541-d3da-2411c9c2a5c9"
      },
      "source": [
        "type(edge_matrix)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Tensor"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FraDRpCXIAbw",
        "outputId": "cbbe5090-d0c9-4444-f01a-6cb685446279"
      },
      "source": [
        "edge_matrix.is_cuda"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUvo8JDPFo_8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc4ca28f-3180-4192-f756-c268bb870543"
      },
      "source": [
        "# incorporate NRC VAD intensity\n",
        "\n",
        "if KB == \"conceptnet\":\n",
        "  logging.info(\"Loading NRC...\")\n",
        "  NRC = load_pickle(\"/content/drive/MyDrive/Proyek Mandiri/data/KB/NRC.pkl\")\n",
        "  affectiveness = np.zeros(vocab_size)\n",
        "  #affectiveness = torch.zeros([vocab_size], dtype=torch.float)\n",
        "  for w, id in vocab.word2id.items():\n",
        "    VAD = get_emotion_intensity(NRC, w)\n",
        "    affectiveness[id] = VAD\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-06-19-09-03-34  INFO  Loading NRC...\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqCd_7aTd7py"
      },
      "source": [
        "#create tensor for affectiveness\n",
        "if KB == \"conceptnet\":\n",
        "  affectiveness = torch.FloatTensor(affectiveness).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n41_yN3gnDcF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7bf3a6a-6bd0-4eb7-ae78-18f4d1165a23"
      },
      "source": [
        "output_size = len(vocab.emotion2id)\n",
        "max_conversation_length_train = len(train[0])\n",
        "max_conversation_length_val = len(val[0])\n",
        "logging.info(\"Number of training utterances: {0}\".format(vocab.num_utterances))\n",
        "logging.info(\"Average number of training utterances per conversation: {0}\".format(vocab.num_utterances/len(train)))\n",
        "logging.info(\"Max conversation length in training set: {0}\".format(max_conversation_length_train))\n",
        "logging.info(\"Max conversation length in validation set: {0}\".format(max_conversation_length_val))\n",
        "logging.info(\"Emotion to ids: {0}\".format(vocab.emotion2id))\n",
        "logging.info(\"Emotion distribution: {0}\".format(vocab.emotion_freq_dist))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-06-19-09-03-42  INFO  Number of training utterances: 87170\n",
            "2021-06-19-09-03-42  INFO  Average number of training utterances per conversation: 7.8404389278647235\n",
            "2021-06-19-09-03-42  INFO  Max conversation length in training set: 35\n",
            "2021-06-19-09-03-42  INFO  Max conversation length in validation set: 31\n",
            "2021-06-19-09-03-42  INFO  Emotion to ids: {'0': 0, '4': 1, '6': 2, '5': 3, '1': 4, '2': 5, '3': 6}\n",
            "2021-06-19-09-03-42  INFO  Emotion distribution: Counter({'0': 72143, '4': 11182, '6': 1600, '5': 969, '1': 827, '2': 303, '3': 146})\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQ8oG-RknrvX"
      },
      "source": [
        "train = convert_examples_to_ids(train, vocab)\n",
        "val = convert_examples_to_ids(val, vocab)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VVwEUbEnx3n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0dfc238f-0b28-4124-9937-1f7bfd9bfb7a"
      },
      "source": [
        "train = flatten_examples_classification(train, vocab, k=context_length)\n",
        "val = flatten_examples_classification(val, vocab, k=context_length)\n",
        "logging.info(\"Batch size: {0}\".format(batch_size))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-06-19-09-04-24  INFO  Batch size: 16\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NoiXX_oNn_ub",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dd434f8-14ae-4cc5-832f-48a6d8aff5e9"
      },
      "source": [
        " # model \n",
        "logging.info(\"Building model...\")\n",
        "model_kwargs = {\n",
        "        \"src_vocab\": vocab_size,\n",
        "        \"tgt_vocab\": vocab_size,\n",
        "        \"N\": n_layers,\n",
        "        \"d_model\": d_model,\n",
        "        \"d_ff\": d_ff,\n",
        "        \"h\": h,\n",
        "        \"output_size\": output_size,\n",
        "        \"dropout\": dropout,\n",
        "        \"KB\": bool(KB),\n",
        "        \"model_variant\": model_variant,\n",
        "        \"context_length\": context_length,\n",
        "        \"graph_attention_variant\": graph_attention_variant\n",
        "}\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-06-19-09-04-29  INFO  Building model...\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QI30sZpoolEt",
        "outputId": "319ff2dd-553b-4959-f890-25531df1fb6f"
      },
      "source": [
        "model = make_model(**model_kwargs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Nilai H : 4\n",
            "Nilai d_model : 300\n",
            "Nilai H : 4\n",
            "Nilai d_model : 300\n",
            "Nilai H : 4\n",
            "Nilai d_model : 300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "By-ECmQEBEUP"
      },
      "source": [
        "#model_iniliatization\n",
        "for p in model.parameters():\n",
        "        if p.dim() > 1:\n",
        "            nn.init.xavier_uniform_(p)\n",
        "    \n",
        "if KB == \"conceptnet\":\n",
        "        if GAW < 0:\n",
        "            GAW = None\n",
        "        model.graph_attention.init_params(GAW, edge_matrix, affectiveness, concentration_factor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vW06LkqchLN5",
        "outputId": "22d959e9-d18d-4ead-f518-42f0c34c844f"
      },
      "source": [
        "logging.info(\"Initializing pretrained word embeddings into transformer...\")\n",
        "model.src_embed[0].embedding.weight.data.copy_(torch.from_numpy(pretrained_word_embedding))\n",
        "model.tgt_embed[0].embedding.weight.data.copy_(torch.from_numpy(pretrained_word_embedding))\n",
        "if KB != \"\":\n",
        "     model.graph_attention.concept_embed.weight.data.copy_(torch.from_numpy(pretrained_word_embedding))\n",
        "logging.info(model)\n",
        "logging.info(\"Number of model params: {0}\".format(count_parameters(model)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-06-19-09-04-41  INFO  Initializing pretrained word embeddings into transformer...\n",
            "2021-06-19-09-04-42  INFO  EncoderDecoder(\n",
            "  (encoder): Encoder(\n",
            "    (layers): ModuleList(\n",
            "      (0): EncoderLayer(\n",
            "        (self_attn): MultiHeadAttention(\n",
            "          (linears): ModuleList(\n",
            "            (0): Linear(in_features=300, out_features=300, bias=True)\n",
            "            (1): Linear(in_features=300, out_features=300, bias=True)\n",
            "            (2): Linear(in_features=300, out_features=300, bias=True)\n",
            "            (3): Linear(in_features=300, out_features=300, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (feed_forward): PositionWiseFeedForward(\n",
            "          (w1): Linear(in_features=300, out_features=400, bias=True)\n",
            "          (w2): Linear(in_features=400, out_features=300, bias=True)\n",
            "          (dropout): Dropout(p=0, inplace=False)\n",
            "        )\n",
            "        (sublayer): ModuleList(\n",
            "          (0): SublayerConnection(\n",
            "            (norm): LayerNorm()\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "          (1): SublayerConnection(\n",
            "            (norm): LayerNorm()\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (norm): LayerNorm()\n",
            "  )\n",
            "  (decoder): Decoder(\n",
            "    (layers): ModuleList(\n",
            "      (0): DecoderLayer(\n",
            "        (self_attn): MultiHeadAttention(\n",
            "          (linears): ModuleList(\n",
            "            (0): Linear(in_features=300, out_features=300, bias=True)\n",
            "            (1): Linear(in_features=300, out_features=300, bias=True)\n",
            "            (2): Linear(in_features=300, out_features=300, bias=True)\n",
            "            (3): Linear(in_features=300, out_features=300, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (src_attn): MultiHeadAttention(\n",
            "          (linears): ModuleList(\n",
            "            (0): Linear(in_features=300, out_features=300, bias=True)\n",
            "            (1): Linear(in_features=300, out_features=300, bias=True)\n",
            "            (2): Linear(in_features=300, out_features=300, bias=True)\n",
            "            (3): Linear(in_features=300, out_features=300, bias=True)\n",
            "          )\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (feed_forward): PositionWiseFeedForward(\n",
            "          (w1): Linear(in_features=300, out_features=400, bias=True)\n",
            "          (w2): Linear(in_features=400, out_features=300, bias=True)\n",
            "          (dropout): Dropout(p=0, inplace=False)\n",
            "        )\n",
            "        (sublayer): ModuleList(\n",
            "          (0): SublayerConnection(\n",
            "            (norm): LayerNorm()\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "          (1): SublayerConnection(\n",
            "            (norm): LayerNorm()\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "          (2): SublayerConnection(\n",
            "            (norm): LayerNorm()\n",
            "            (dropout): Dropout(p=0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (norm): LayerNorm()\n",
            "  )\n",
            "  (src_embed): Sequential(\n",
            "    (0): Embeddings(\n",
            "      (embedding): Embedding(19447, 300)\n",
            "    )\n",
            "    (1): PositionalEncoding(\n",
            "      (dropout): Dropout(p=0, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (tgt_embed): Sequential(\n",
            "    (0): Embeddings(\n",
            "      (embedding): Embedding(19447, 300)\n",
            "    )\n",
            "    (1): PositionalEncoding(\n",
            "      (dropout): Dropout(p=0, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (generator): Generator(\n",
            "    (proj): Linear(in_features=300, out_features=7, bias=True)\n",
            "  )\n",
            "  (graph_attention): GraphAttention(\n",
            "    (concept_embed): Embedding(19447, 300)\n",
            "  )\n",
            "  (src_mlp): Linear(in_features=600, out_features=300, bias=True)\n",
            ")\n",
            "2021-06-19-09-04-42  INFO  Number of model params: 13439260\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSi9OQ7WhhDq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5109d93e-a4be-4b32-b400-038db2763b70"
      },
      "source": [
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "EncoderDecoder(\n",
              "  (encoder): Encoder(\n",
              "    (layers): ModuleList(\n",
              "      (0): EncoderLayer(\n",
              "        (self_attn): MultiHeadAttention(\n",
              "          (linears): ModuleList(\n",
              "            (0): Linear(in_features=300, out_features=300, bias=True)\n",
              "            (1): Linear(in_features=300, out_features=300, bias=True)\n",
              "            (2): Linear(in_features=300, out_features=300, bias=True)\n",
              "            (3): Linear(in_features=300, out_features=300, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (feed_forward): PositionWiseFeedForward(\n",
              "          (w1): Linear(in_features=300, out_features=400, bias=True)\n",
              "          (w2): Linear(in_features=400, out_features=300, bias=True)\n",
              "          (dropout): Dropout(p=0, inplace=False)\n",
              "        )\n",
              "        (sublayer): ModuleList(\n",
              "          (0): SublayerConnection(\n",
              "            (norm): LayerNorm()\n",
              "            (dropout): Dropout(p=0, inplace=False)\n",
              "          )\n",
              "          (1): SublayerConnection(\n",
              "            (norm): LayerNorm()\n",
              "            (dropout): Dropout(p=0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (norm): LayerNorm()\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (layers): ModuleList(\n",
              "      (0): DecoderLayer(\n",
              "        (self_attn): MultiHeadAttention(\n",
              "          (linears): ModuleList(\n",
              "            (0): Linear(in_features=300, out_features=300, bias=True)\n",
              "            (1): Linear(in_features=300, out_features=300, bias=True)\n",
              "            (2): Linear(in_features=300, out_features=300, bias=True)\n",
              "            (3): Linear(in_features=300, out_features=300, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (src_attn): MultiHeadAttention(\n",
              "          (linears): ModuleList(\n",
              "            (0): Linear(in_features=300, out_features=300, bias=True)\n",
              "            (1): Linear(in_features=300, out_features=300, bias=True)\n",
              "            (2): Linear(in_features=300, out_features=300, bias=True)\n",
              "            (3): Linear(in_features=300, out_features=300, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (feed_forward): PositionWiseFeedForward(\n",
              "          (w1): Linear(in_features=300, out_features=400, bias=True)\n",
              "          (w2): Linear(in_features=400, out_features=300, bias=True)\n",
              "          (dropout): Dropout(p=0, inplace=False)\n",
              "        )\n",
              "        (sublayer): ModuleList(\n",
              "          (0): SublayerConnection(\n",
              "            (norm): LayerNorm()\n",
              "            (dropout): Dropout(p=0, inplace=False)\n",
              "          )\n",
              "          (1): SublayerConnection(\n",
              "            (norm): LayerNorm()\n",
              "            (dropout): Dropout(p=0, inplace=False)\n",
              "          )\n",
              "          (2): SublayerConnection(\n",
              "            (norm): LayerNorm()\n",
              "            (dropout): Dropout(p=0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (norm): LayerNorm()\n",
              "  )\n",
              "  (src_embed): Sequential(\n",
              "    (0): Embeddings(\n",
              "      (embedding): Embedding(19447, 300)\n",
              "    )\n",
              "    (1): PositionalEncoding(\n",
              "      (dropout): Dropout(p=0, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (tgt_embed): Sequential(\n",
              "    (0): Embeddings(\n",
              "      (embedding): Embedding(19447, 300)\n",
              "    )\n",
              "    (1): PositionalEncoding(\n",
              "      (dropout): Dropout(p=0, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (generator): Generator(\n",
              "    (proj): Linear(in_features=300, out_features=7, bias=True)\n",
              "  )\n",
              "  (graph_attention): GraphAttention(\n",
              "    (concept_embed): Embedding(19447, 300)\n",
              "  )\n",
              "  (src_mlp): Linear(in_features=600, out_features=300, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3R-hJfsvmEZ",
        "outputId": "f1942e2d-b2c0-43af-83d2-dfce6fc67234"
      },
      "source": [
        "# weighted crossentropy loss\n",
        "logging.info(\"Computing label weights...\")\n",
        "label_weight = np.array(label_distribution_transformer(val))/np.array(label_distribution_transformer(train))\n",
        "label_weight = torch.tensor(label_weight/label_weight.sum()).float().to(device)*output_size\n",
        "logging.info(\"Label weight: {0}\".format(label_weight))\n",
        "criterion = nn.CrossEntropyLoss(weight=label_weight, reduction=\"sum\")\n",
        "optimizer = optim.Adam(model.parameters(), lr)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-06-19-09-04-52  INFO  Computing label weights...\n",
            "2021-06-19-09-04-52  INFO  Label weight: tensor([1.4178, 0.8802, 0.9623, 1.1732, 1.3398, 0.1425, 1.0842],\n",
            "       device='cuda:0')\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eK7ZcoIAjrCp"
      },
      "source": [
        "#pake model epoch 8 dari previous training\n",
        "model_save_name = 'classifier2.pt'\n",
        "path = F\"/content/drive/MyDrive/Proyek Mandiri/checkpint3/{model_save_name}\"\n",
        "model.load_state_dict(torch.load(path))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EKh5IeJvupT"
      },
      "source": [
        "# training\n",
        "train_epoch_losses = []\n",
        "val_epoch_losses = []\n",
        "logging.info(\"Start training...\")\n",
        "for epoch in range(1, epochs + 1):\n",
        "        train_batches = create_batches_classification(train, batch_size, vocab, train=True)\n",
        "        val_batches = create_batches_classification(val, batch_size, vocab, train=False)\n",
        "        \n",
        "        train_epoch_loss = []\n",
        "        val_epoch_loss = []\n",
        "        model.train()\n",
        "        loss_compute = SimpleLossCompute(model.generator, criterion, dataset, vocab.emotion2id, opt=optimizer, test=test_mode)\n",
        "\n",
        "        for batch in train_batches:\n",
        "            batch.to(device)\n",
        "            out = model.forward(batch.src, batch.tgt, \n",
        "                                batch.src_mask, batch.tgt_mask)\n",
        "            loss = loss_compute(out, batch.tgt_y, batch.ntokens)\n",
        "            train_epoch_loss.append((loss/batch.ntokens).item())\n",
        "        logging.info(\"-\"*80)\n",
        "        logging.info(\"Epoch {0}/{1}\".format(epoch, epochs))\n",
        "        logging.info(\"Training loss: {0:.4f}\".format(np.mean(train_epoch_loss)))\n",
        "        train_epoch_losses.append(np.mean(train_epoch_loss))\n",
        "        score = loss_compute.score()\n",
        "        loss_compute.clear()\n",
        "\n",
        "        # validation\n",
        "        # get src_attn\n",
        "        src_attns = []\n",
        "        model.eval()\n",
        "        loss_compute = SimpleLossCompute(model.generator, criterion, dataset, vocab.emotion2id, opt=None, test=test_mode)\n",
        "        with torch.no_grad():\n",
        "            for batch in val_batches:\n",
        "                batch.to(device)\n",
        "                out = model.forward(batch.src, batch.tgt, \n",
        "                                    batch.src_mask, batch.tgt_mask)\n",
        "                # get src attn\n",
        "                src_attns.append(model.decoder.layers[0].src_attn.attn)\n",
        "                loss = loss_compute(out, batch.tgt_y, batch.ntokens)\n",
        "                val_epoch_loss.append((loss/batch.ntokens).item())\n",
        "            logging.info(\"Validation loss: {0:.4f}\".format(np.mean(val_epoch_loss)))\n",
        "            val_epoch_losses.append(np.mean(val_epoch_loss))\n",
        "\n",
        "        # get validation metrics\n",
        "        score = loss_compute.score()\n",
        "        loss_compute.clear()\n",
        "        logging.info(\"Validation score: {0}\".format(score))\n",
        "\n",
        "        #save the model\n",
        "        model_save_name = 'classifier{0}.pt'.format(epoch)\n",
        "        path = F\"/content/drive/MyDrive/Proyek Mandiri/checkpint4/{model_save_name}\" \n",
        "        torch.save(model.state_dict(), path)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}